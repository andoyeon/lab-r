rm(lm_model2)
# 각 모델(model2, model5)에서 예측 결과와 실제 strength간의 상관 계수를 계산
# 노드가 하나일 때 : 0.8
model2_result <- compute(model2, concrete_test[-9])
head(model2_result)
summary(model2_result)
predict2 <- model2_result$net.result
cor(predict2, concrete_test$strength)
model5_result <- compute(model5, concrete_test[-9])
head(model5_result)
summary(model5_result)
predict5 <- model5_result$net.result
cor(predict5, concrete_test$strength)
# 평균 절대 오차(MAE: Mean Absolute Error) 함수 작성
# -> 각 모델의 MAE를 계산
MAE <- function(actual, predict) {
return(mean(abs(actual - predict)))
}
MAE(actual = concrete_test$strength, predict = predict2)
MAE(actual = concrete_test$strength, predict = predict5)
# 역정규화: (정규화 ->  실제값) 함수 작성
# -> 실제 데이터 프레임(concrete)의 값들과 비교
# normalization = (x - min) / (max - min)
# x = normalization * (max - min) + min
denormalization <- function(x) {
return(normalization * (max(x) - min(x)) + min(x))
}
summary(concrete_model)
concrete_denorm <- as.data.frame(lapply(concrete_model, denormalization))
concrete_denorm <- as.data.frame(lapply(model_result, denormalization))
str(concrete)
concrete_denorm <- as.data.frame(lapply(concretet, denormalization))
concrete_denorm <- as.data.frame(lapply(concretet_norm, denormalization))
concrete_denorm <- as.data.frame(lapply(concrete, denormalization))
concrete_denorm <- as.data.frame(lapply(concrete_norm, denormalization))
concrete_denorm <- as.data.frame(lapply(concrete_model, denormalization))
concrete_denorm <- as.data.frame(lapply(model_result, denormalization))
concrete_denorm <- as.data.frame(lapply(predict_result, denormalization))
return(x * (max(x) - min(x)) + min(x))
concrete_denorm <- as.data.frame(lapply(concrete_norm, denormalization))
# 역정규화: (정규화 ->  실제값) 함수 작성
# -> 실제 데이터 프레임(concrete)의 값들과 비교
# normalization = (x - min) / (max - min)
# x = normalization * (max - min) + min
denormalization <- function(x) {
return(normalization() * (max(x) - min(x)) + min(x))
}
concrete_denorm <- as.data.frame(lapply(concrete_norm, denormalization))
# 역정규화: (정규화 ->  실제값) 함수 작성
# -> 실제 데이터 프레임(concrete)의 값들과 비교
# normalization = (x - min) / (max - min)
# x = normalization * (max - min) + min
denormalization <- function(x) {
return(normalization(x) * (max(x) - min(x)) + min(x))
}
concrete_denorm <- as.data.frame(lapply(concrete_norm, denormalization))
summary(concrete_norm)
str(concrete_norm)
summary(concrete_norm)
summary(concrete_norm)
# 역정규화: (정규화 ->  실제값) 함수 작성
# -> 실제 데이터 프레임(concrete)의 값들과 비교
# normalization = (x - min) / (max - min)
# x = normalization * (max - min) + min
denormalization <- function(x) {
max_str <- max(concrete$strength)
min_str <- min(concrete$strength)
return(normalization * (max_str - min_str) + min_str)
}
concrete_denorm <- as.data.frame(lapply(concrete_norm, denormalization))
# 역정규화: (정규화 ->  실제값) 함수 작성
# -> 실제 데이터 프레임(concrete)의 값들과 비교
# normalization = (x - min) / (max - min)
# x = normalization * (max - min) + min
denormalization <- function(x) {
max_str <- max(concrete$strength)
min_str <- min(concrete$strength)
return(x * (max_str - min_str) + min_str)
}
concrete_denorm <- as.data.frame(lapply(concrete_norm, denormalization))
summary(concrete_norm)
summary(concrete)
summary(concrete_denorm)
head(concrete)
head(concrete_denorm)
summary(concrete_denorm$strength)
summary(concrete$strength)
head(concrete$strength)
head(concrete_denorm$strength)
rm(list = ls())
concrete <- read.csv(file = "mlwr/concrete.csv")
# 데이터 확인
str(concrete)
summary(concrete)
# 정규화
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
# 신경망 모델 생성, 학습
# 1) 학습 데이터 세트(75%), 테스트 데이터 세트(25%)
1030 * 0.75
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:103, ]
concrete_test <- concrete_norm[774:1030, ]
summary(concrete_train)
summary(concrete_test)
summary(concrete_train)
summary(concrete_test)
# 신경망 모델 생성: neuralnet
concrete_model <- neuralnet(formula = strength ~., data = concrete_train)
# 생성된 NN 확인: plot
plot(concrete_model)
# 만들어진 NN 평가 -  테스트 데이터 세트에 적용: compute
model_result <- compute(concrete_model, concrete_norm[-9])
# 만들어진 NN 평가 -  테스트 데이터 세트에 적용: compute
concrete_result <- compute(concrete_model, concrete_test[-9])
head(concrete_result)
summary(concrete_result)
predict_result <- concrete_result$net.result
# 예측결과와 실제 값의 상관 관계 = 상관 계수
cor(prect_result, concrete_test$strength)
# 예측결과와 실제 값의 상관 관계 = 상관 계수
cor(predict_result, concrete_test$strength)
concrete_test[255:257, 9]
# 모델 향상
concrete_model5 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = 5)
plot(concrete_model5)
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2))
plot(concrete_model3)
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2),
act.fct = "logical")
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2),
act.fct = "logistic")
plot(concrete_model3)
plot(concrete_model3)
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2),
act.fct = "tanh")
plot(concrete_model3)
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2),
act.fct = "tanh")
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2),
act.fct = "tanh",
col = "blue")
plot(concrete_model3)
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2),
act.fct = "tanh",
col = "darkblue")
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2),
act.fct = "logistic")
plot(concrete_model3, col = "red")
plot(concrete_model3, color = "red")
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(3,2),
act.fct = "logistic")
plot(concrete_model3)
# 신경망 모델 평가
model3_result <- compute(concrete_model3, concrete_test[-9])
model3_predict <- model3_result$net.result
cor(model3_predict, concrete_test$strength)
# 평균 절대 오차(MAE: Mean Absolute Error) 함수 작성
# -> 각 모델의 MAE를 계산
MAE <- function(actual, predict) {
return(mean(abs(actual - predict)))
}
MAE(actual = conctrete_test$strength, predict = model3_predict)
MAE(actual = concrete_test$strength, predict = model3_predict)
plot(concrete_model3)
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5,2),
act.fct = "logistic")
plot(concrete_model3)
# 신경망 모델 평가
model3_result <- compute(concrete_model3, concrete_test[-9])
model3_predict <- model3_result$net.result
cor(model3_predict, concrete_test$strength)   # 0.921
# 다른 신경망 모델
concrete_model3 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(6,3),
act.fct = "logistic")
plot(concrete_model3)
# 신경망 모델 평가
model3_result <- compute(concrete_model3, concrete_test[-9])
model3_predict <- model3_result$net.result
cor(model3_predict, concrete_test$strength)   # 0.921
MAE(actual = concrete_test$strength, predict = model3_predict)  # 0.06
# 2. 데이터 확인, 전처리
str(concrete)
l <- lapply(concrete, normalization)
normalization <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
l <- lapply(concrete, normalization)
l[[1]]
summary(concrete_norm)
l$strength
rm(l)
summary(concrete_norm)
concrete_model <- neuralnet(formula = strength ~ .,
data = concrete_train)
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
summary(concrete_train$strength)
summary(concrete_test$strength)
# 신경망 모델 생성
set.seed(12345)
concrete_model <- neuralnet(formula = strength ~ .,
data = concrete_train)
library(neuralnet)
concrete_model <- neuralnet(formula = strength ~ .,
data = concrete_train)
# 생성된 NN을 확인
plot(concrete_model)
View(concrete_result)
# 4. 만들어진 NN을 평가 - 테스트 데이터 세트에 적용
model_result <- compute(concrete_model, concrete_test[-9])
predict_result <- model_result$net.result
# 예측 결과와 실제 값의 상관 관계 = 상관 계수
cor(predict_result, concrete_test$strength)   # 0.8
concrete_test[255:257, 9]   # 실제값
# 5. 모델 향상
model2 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = 2)
plot(model2)
model5 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = 5)
plot(model5)
# 평균 절대 오차(MAE: Mean Absolute Error) 함수 작성
# -> 각 모델의 MAE를 계산
MAE <- function(actual, predict) {
return(mean(abs(actual - predict)))
}
MAE(actual = concrete_test$strength, predict = model_result)
# 4. 만들어진 NN을 평가 - 테스트 데이터 세트에 적용
model_result <- compute(concrete_model, concrete_test[-9])
MAE(actual = concrete_test$strength, predict = model_result)
# 평균 절대 오차(MAE: Mean Absolute Error) 함수 작성
# -> 각 모델의 MAE를 계산
MAE <- function(actual, predict) {
return(mean(abs(actual - predict)))
}
MAE(actual = concrete_test$strength, predict = model_result)
concrete_model <- neuralnet(formula = strength ~ .,
data = concrete_train)
# 4. 만들어진 NN을 평가 - 테스트 데이터 세트에 적용
model_result <- compute(concrete_model, concrete_test[-9])
predict_result <- model_result$net.result
# 예측 결과와 실제 값의 상관 관계 = 상관 계수
cor(predict_result, concrete_test$strength)   # 0.8
# 5. 모델 향상
model2 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = 2)
model5 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = 5)
# 각 모델(model2, model5)에서 예측 결과와 실제 strength간의 상관 계수를 계산
# 노드가 하나일 때 : 0.8
model2_result <- compute(model2, concrete_test[-9])
predict2 <- model2_result$net.result
model5_result <- compute(model5, concrete_test[-9])
predict5 <- model5_result$net.result
cor(predict5, concrete_test$strength)   # 0.928
# 평균 절대 오차(MAE: Mean Absolute Error) 함수 작성
# -> 각 모델의 MAE를 계산
MAE <- function(actual, predict) {
return(mean(abs(actual - predict)))
}
MAE(actual = concrete_test$strength, predict = model_result)
MAE(actual = concrete_test$strength, predict = predict2)    # 0.068
MAE(actual = concrete_test$strength, predict = predict5)    # 0.057
MAE(actual = concrete_test$strength, predict = predict_result)
# 역정규화: (정규화 ->  실제값) 함수 작성
# -> 실제 데이터 프레임(concrete)의 값들과 비교
# normalization = (x - min) / (max - min)
# x = normalization * (max - min) + min
denormalization <- function(x) {
max_str <- max(concrete$strength)
min_str <- min(concrete$strength)
return(x * (max_str - min_str) + min_str)
}
predict_denorm <- denormalization(predict_result)
rm(predict_denorm)
summary(predict1_denorm)
predict1_denorm <- denormalization(predict_result)
summary(predict1_denorm)
predict2_denorm <- denormalization(predict2)
summary(predict2_denorm)
predict5_denorm <- denormalization(predict5)
summary(predict5)
predict5_denorm <- denormalization(predict5)
summary(predict5)
model5_result <- compute(model5, concrete_test[-9])
model5 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = 5)
model5_result <- compute(model5, concrete_test[-9])
predict5 <- model5_result$net.result
predict5_denorm <- denormalization(predict5)
summary(predict5)
actual_predict_df <- data.frame(autual = concrete[774:1030, 9],
predict1 = predict1_denorm,
predict2 = predict2_denorm,
predict5 = predict5_denorm)
head(actual_predict_df)
actual_predict_df <- data.frame(actual = concrete[774:1030, 9],
predict1 = predict1_denorm,
predict2 = predict2_denorm,
predict5 = predict5_denorm)
cor(actual_predict_df$actual, actual_predict_df$predict1)
cor(actual_predict_df$actual, actual_predict_df$predict2)   # 0.8
cor(actual_predict_df$actual, actual_predict_df$predict5)   # 0.8
# neuralnet 함수의 파라미터 중에서
# hidden 파라미터는 은닉 노드와 은닉 계층의 갯수를 조정할 수 있고,
# act.fct 파라미터는 활성 함수를 바꿔줄 수 있습니다.
# 두 개의 파라미터를 활용해서 다른 신경망 모델을 만들어 보고,
# 예측 결과가 얼마나 개선되는지 확인해 보세요.
set.seed(12345)
softsum <- function(x) {
log(1 + exp(x))
}
curve(expr = softsum, from = -5, to = 5)
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 3),
act.fct = softsum())
act.fct = softsum
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 3),
act.fct = softsum)
curve(expr = softsum, from = -5, to = 5)
curve(expr = softsum(x), from = -5, to = 5)
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 3),     # c(은닉 노드 갯수, 은닉층 갯수)
act.fct = softsum(x))    # 활성 함수
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 5),     # c(첫번째 층 노드 갯수, 두번째 층 노드 갯수)
act.fct = softsum)    # 활성 함수
plot(model)
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 5),     # c(첫번째 층 노드 갯수, 두번째 층 노드 갯수)
act.fct = softsum,
stepmax = 1e6)    # 활성 함수
plot(model)
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 3),     # c(첫번째 층 노드 갯수, 두번째 층 노드 갯수)
act.fct = softsum,    # 활성 함수
stepmax = 1e6)
plot(model)
rm(list = ls())
# 1. 데이터 준비
letters <- read.csv(file = "mlwr/letterdata.csv")
# 2. 데이터 확인, 전처리
str(letters)
head(letters)
table(letters$letter)
# 3. 학습 데이터(80%) / 테스트 데이트(20%) 세트
letters_train <- letters[1:16000, ]
letters_test <- letters[16001:20000, ]
table(letters_train)
table(letters_train$letter)
table(letters_test$letter)
# 3. 모델 생성 - SVM
# kernlab 패키지
install.packages("kernlab")
library(kernlab)
search()
detach("package:neuralnet")
search()
# SVM 알고리즘 모델을 생성
letter_classifier <- ksvm(letter ~ .,
data = letters_train,
kernel = "vanilladot")
# 4. 모델 평가
letters_predict <- predict(letter_classifier, letters_test)
head(letters_predict)
table(letters_predict, letters_test$letter)
prop.table(table(letters_predict, letters_test$letter))
letters_predict[1] == letters_test$letter[1]
correct_count <- letters_predict == letters$letter
table(correct_count)
prop.table(table(correct_count))
correct_count <- ifelse(letters_predict == letters_test$letter, 1, 0)
correct_count
correct <- ifelse(letters_predict == letters_test$letter, 1, 0)
correct
correct_count <- sum(correct)
correct_count
correct_ratio <- correct_count / 4000
correct_ratio
# 5. 모델 수정 -> 재평가 -> 성능 개선
classifier2 <- ksvm(letter ~ .,
data = letters_train,
kernel = "rbfdot")
predict2 <- predict(classifier2, letters_test)
head(predict2)
head(predict2, n = 10)
head(letters_test$letter, n = 10)
View(letters_test)
table(predict2, letters_test$letter)
correct2_sum <- sum(correct2)
correct2_sum
correct2 <- ifelse(predict2 == letters_test$letter, 1, 0)
correct2_count <- sum(correct2)
correct2_sum
correct2_count
correct2_ratio <- correct2_count / 4000
correct_ratio
correct2 <- ifelse(predict2 == letters_test$letter, 1, 0)
correct2_count <- sum(correct2)
correct2_count
correct_count   # SVM 모델이 문자들을 제대로 구분한 갯수
# 5. 모델 수정 -> 재평가 -> 성능 개선
classifier2 <- ksvm(letter ~ .,
data = letters_train,
kernel = "robot")
# 5. 모델 수정 -> 재평가 -> 성능 개선
classifier2 <- ksvm(letter ~ .,
data = letters_train,
kernel = "rbfdot")
predict2 <- predict(classifier2, letters_test)
head(predict2, n = 10)
head(predict2, n = 10)
head(letters_test$letter, n = 10)
correct2 <- ifelse(predict2 == letters_test$letter, 1, 0)
correct2_count <- sum(correct2)
correct2_count
correct2_ratio <- correct2_count / 4000
correct_ratio
correct2_ratio <- correct2_count / 4000
correct_ratio
correct2_ratio
table(predict2, letters_test$letter)
rm(list = ls())
# 1. 데이터 준비
letters <- read.csv(file = "mlwr/letterdata.csv")
# 2. 데이터 확인
str(letters)
head(letter)
head(letters)
# 학습/테스트 데이터 세트 (80%:20%)
letter_train <- letters[1:1600, ]
# 학습/테스트 데이터 세트 (80%:20%)
letter_train <- letters[1:16000, ]
letter_test <- letters[16001:20000]
letter_test <- letters[16001:20000, ]
table(letter_train)
table(letter_train$letter)
table(letter_test$letter)
# 3. SVM 알고리즘 모델 생성: classifier
classifier <- ksvm(letter ~ .,
data = letter_train,
kernel = "rbfdot")
head(classifier)
# 4. 모델 평가
letters_predict <- predict(classifier, letters_test)
# 4. 모델 평가
letters_predict <- predict(classifier, letter_test)
head(letters_predict)
letter_test[ ,1]
head(letter_test$letter, n = 10)
head(letters_predict)
head(letter_test$letter, n = 10)
head(letters_predict, n = 10)
table(letter_test, letters_predict)
table(letter_test$letter, letters_predict)
correct <- ifelse(letters_predict === letter_test$letter, 1, 0)
correct <- ifelse(letters_predict == letter_test$letter, 1, 0)
table(correct)
correct
correct_sum <- sum(correct)
correct_sum
correct_ratio <- correct_sum / 4000
correct_ratio
