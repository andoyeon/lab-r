# normalization = (x - min) / (max - min)
# x = normalization * (max - min) + min
denormalization <- function(x) {
max_str <- max(concrete$strength)
min_str <- min(concrete$strength)
return(x * (max_str - min_str) + min_str)
}
predict_denorm <- denormalization(predict_result)
rm(predict_denorm)
summary(predict1_denorm)
predict1_denorm <- denormalization(predict_result)
summary(predict1_denorm)
predict2_denorm <- denormalization(predict2)
summary(predict2_denorm)
predict5_denorm <- denormalization(predict5)
summary(predict5)
predict5_denorm <- denormalization(predict5)
summary(predict5)
model5_result <- compute(model5, concrete_test[-9])
model5 <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = 5)
model5_result <- compute(model5, concrete_test[-9])
predict5 <- model5_result$net.result
predict5_denorm <- denormalization(predict5)
summary(predict5)
actual_predict_df <- data.frame(autual = concrete[774:1030, 9],
predict1 = predict1_denorm,
predict2 = predict2_denorm,
predict5 = predict5_denorm)
head(actual_predict_df)
actual_predict_df <- data.frame(actual = concrete[774:1030, 9],
predict1 = predict1_denorm,
predict2 = predict2_denorm,
predict5 = predict5_denorm)
cor(actual_predict_df$actual, actual_predict_df$predict1)
cor(actual_predict_df$actual, actual_predict_df$predict2)   # 0.8
cor(actual_predict_df$actual, actual_predict_df$predict5)   # 0.8
# neuralnet 함수의 파라미터 중에서
# hidden 파라미터는 은닉 노드와 은닉 계층의 갯수를 조정할 수 있고,
# act.fct 파라미터는 활성 함수를 바꿔줄 수 있습니다.
# 두 개의 파라미터를 활용해서 다른 신경망 모델을 만들어 보고,
# 예측 결과가 얼마나 개선되는지 확인해 보세요.
set.seed(12345)
softsum <- function(x) {
log(1 + exp(x))
}
curve(expr = softsum, from = -5, to = 5)
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 3),
act.fct = softsum())
act.fct = softsum
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 3),
act.fct = softsum)
curve(expr = softsum, from = -5, to = 5)
curve(expr = softsum(x), from = -5, to = 5)
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 3),     # c(은닉 노드 갯수, 은닉층 갯수)
act.fct = softsum(x))    # 활성 함수
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 5),     # c(첫번째 층 노드 갯수, 두번째 층 노드 갯수)
act.fct = softsum)    # 활성 함수
plot(model)
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 5),     # c(첫번째 층 노드 갯수, 두번째 층 노드 갯수)
act.fct = softsum,
stepmax = 1e6)    # 활성 함수
plot(model)
model <- neuralnet(formula = strength ~ .,
data = concrete_train,
hidden = c(5, 3),     # c(첫번째 층 노드 갯수, 두번째 층 노드 갯수)
act.fct = softsum,    # 활성 함수
stepmax = 1e6)
plot(model)
rm(list = ls())
# 1. 데이터 준비
letters <- read.csv(file = "mlwr/letterdata.csv")
# 2. 데이터 확인, 전처리
str(letters)
head(letters)
table(letters$letter)
# 3. 학습 데이터(80%) / 테스트 데이트(20%) 세트
letters_train <- letters[1:16000, ]
letters_test <- letters[16001:20000, ]
table(letters_train)
table(letters_train$letter)
table(letters_test$letter)
# 3. 모델 생성 - SVM
# kernlab 패키지
install.packages("kernlab")
library(kernlab)
search()
detach("package:neuralnet")
search()
# SVM 알고리즘 모델을 생성
letter_classifier <- ksvm(letter ~ .,
data = letters_train,
kernel = "vanilladot")
# 4. 모델 평가
letters_predict <- predict(letter_classifier, letters_test)
head(letters_predict)
table(letters_predict, letters_test$letter)
prop.table(table(letters_predict, letters_test$letter))
letters_predict[1] == letters_test$letter[1]
correct_count <- letters_predict == letters$letter
table(correct_count)
prop.table(table(correct_count))
correct_count <- ifelse(letters_predict == letters_test$letter, 1, 0)
correct_count
correct <- ifelse(letters_predict == letters_test$letter, 1, 0)
correct
correct_count <- sum(correct)
correct_count
correct_ratio <- correct_count / 4000
correct_ratio
# 5. 모델 수정 -> 재평가 -> 성능 개선
classifier2 <- ksvm(letter ~ .,
data = letters_train,
kernel = "rbfdot")
predict2 <- predict(classifier2, letters_test)
head(predict2)
head(predict2, n = 10)
head(letters_test$letter, n = 10)
View(letters_test)
table(predict2, letters_test$letter)
correct2_sum <- sum(correct2)
correct2_sum
correct2 <- ifelse(predict2 == letters_test$letter, 1, 0)
correct2_count <- sum(correct2)
correct2_sum
correct2_count
correct2_ratio <- correct2_count / 4000
correct_ratio
correct2 <- ifelse(predict2 == letters_test$letter, 1, 0)
correct2_count <- sum(correct2)
correct2_count
correct_count   # SVM 모델이 문자들을 제대로 구분한 갯수
# 5. 모델 수정 -> 재평가 -> 성능 개선
classifier2 <- ksvm(letter ~ .,
data = letters_train,
kernel = "robot")
# 5. 모델 수정 -> 재평가 -> 성능 개선
classifier2 <- ksvm(letter ~ .,
data = letters_train,
kernel = "rbfdot")
predict2 <- predict(classifier2, letters_test)
head(predict2, n = 10)
head(predict2, n = 10)
head(letters_test$letter, n = 10)
correct2 <- ifelse(predict2 == letters_test$letter, 1, 0)
correct2_count <- sum(correct2)
correct2_count
correct2_ratio <- correct2_count / 4000
correct_ratio
correct2_ratio <- correct2_count / 4000
correct_ratio
correct2_ratio
table(predict2, letters_test$letter)
rm(list = ls())
# 1. 데이터 준비
letters <- read.csv(file = "mlwr/letterdata.csv")
# 2. 데이터 확인
str(letters)
head(letter)
head(letters)
# 학습/테스트 데이터 세트 (80%:20%)
letter_train <- letters[1:1600, ]
# 학습/테스트 데이터 세트 (80%:20%)
letter_train <- letters[1:16000, ]
letter_test <- letters[16001:20000]
letter_test <- letters[16001:20000, ]
table(letter_train)
table(letter_train$letter)
table(letter_test$letter)
# 3. SVM 알고리즘 모델 생성: classifier
classifier <- ksvm(letter ~ .,
data = letter_train,
kernel = "rbfdot")
head(classifier)
# 4. 모델 평가
letters_predict <- predict(classifier, letters_test)
# 4. 모델 평가
letters_predict <- predict(classifier, letter_test)
head(letters_predict)
letter_test[ ,1]
head(letter_test$letter, n = 10)
head(letters_predict)
head(letter_test$letter, n = 10)
head(letters_predict, n = 10)
table(letter_test, letters_predict)
table(letter_test$letter, letters_predict)
correct <- ifelse(letters_predict === letter_test$letter, 1, 0)
correct <- ifelse(letters_predict == letter_test$letter, 1, 0)
table(correct)
correct
correct_sum <- sum(correct)
correct_sum
correct_ratio <- correct_sum / 4000
correct_ratio
rm(list = ls())
# 1. 데이터 준비
groceries <- read.csv(file = "mlwr/groceries.csv)
# 1. 데이터 준비
groceries <- read.csv(file = "mlwr/groceries.csv")
# 1. 데이터 준비
groceries <- read.csv(file = "mlwr/groceries.csv")
str(groceries)
head(groceries)
# 1. 데이터 준비
groceries <- read.csv(file = "mlwr/groceries.csv", header = F)
str(groceries)
head(groceries)
tail(groceries)
head(groceries, n = 10)
# 1. 데이터 준비
groceries <- read.csv(file = "mlwr/groceries.csv", header = F)
str(groceries)
head(groceries, n = 10)
# arules 패키지: association rules(연관 규칙) 패키지
install.packages("arules")
library(arules)
search()
# 장바구니 영수증 데이터(csv)를 희소 행렬로 만듦
groceries <- read.transactions(file = "mlwr/groceries.csv",
header = F,
sep = ",")
# read.transaction 함수에서
# header 파라미터의 기본값은 FALSE
# sep 파라미터의 기본값은 ""이기 때문에, 반드시 ","를 전달해야 함.
summary(groceries)
inspect(groceries)
inspect(groceries[1:5])
inspect(groceries[9831:9835])
inspect(groceries[ ,1:5])
# 영수증에 등장하는 아이템들의 빈도(frequency)
itemFrequency(groceries[, 1:5])
itemFrequency(groceries[, 165:169])
# 거래 아이템들의 빈도 분포
itemFrequencyPlot(groceries, support = 0.1)
# support: 영수증에 아이템이 나타나는 횟수
# 최소 10% 이상 나타나는 아이템들만 그래프에 표시
itemFrequencyPlot(groceries, topN = 20)
# 희소 행렬(sparse matrix)를 그래프로 표시
image(groceries[1:100, ])
# 3. 모델 학습 - 자율(비지도) 학습인 한 종류 a priori 알고리즘
grocery_rules <- apriori(data = groceries)
summary(grocery_rules)
300 / 9835
grocery_rules2 <- apriori(data = groceries,
parameter = list(support = 0.03,
confidence = 0.25,
minlen = 2))
summary(grocery_rules2)
inspect(grocery_rules2)
inspect(grocery_rules2)
60 / 9835
grocery_rules3 <- apriori(data = groceries,
parameter = list(support = 0.006,
confidence = 0.25,
minlen = 2))
summary(grocery_rules3)
inspect(grocery_rules3[1:5])
inspect(grocery_rules3[1:10])
inspect(grocery_rules3[1:10])
inspect(grocery_rules3[1:10])
inspect(grocery_rules3[400:410])
inspect(grocery_rules3[400:410])
summary(grocery_rules)
# 1. 데이터 준비
groceries <- read.csv(file = "mlwr/groceries.csv", header = F)
# 장바구니 영수증 데이터(csv)를 희소 행렬로 만듦
groceries <- read.transactions(file = "mlwr/groceries.csv",
header = F,
sep = ",")
# read.transaction 함수에서
# header 파라미터의 기본값은 FALSE
# sep 파라미터의 기본값은 ""이기 때문에, 반드시 ","를 전달해야 함.
summary(groceries)
# 각 영수증(transaction/example)의 아이템들을 확인
inspect(groceries)
inspect(groceries[1:5])
inspect(groceries[9831:9835])
# 영수증에 등장하는 아이템들의 빈도(frequency)
itemFrequency(groceries[, 1:5])
itemFrequency(groceries[, 165:169])
# 거래 아이템들의 빈도 분포
itemFrequencyPlot(groceries, support = 0.1)
# support: 영수증에 아이템이 나타나는 횟수
# 최소 10% 이상 나타나는 아이템들만 그래프에 표시
itemFrequencyPlot(groceries, topN = 20)
# 희소 행렬(sparse matrix)를 그래프로 표시
image(groceries[1:100, ])
# 3. 모델 학습 - 자율(비지도) 학습인 한 종류 a priori 알고리즘
grocery_rules <- apriori(data = groceries)
summary(grocery_rules)
grocery_rules2 <- apriori(data = groceries,
parameter = list(support = 0.03,
confidence = 0.25,
minlen = 2))
summary(grocery_rules2)
grocery_rules2 <- apriori(data = groceries,
parameter = list(support = 0.03,
confidence = 0.25,
minlen = 2))
summary(grocery_rules2)
inspect(grocery_rules2)
grocery_rules3 <- apriori(data = groceries,
parameter = list(support = 0.006,
confidence = 0.25,
minlen = 2))
summary(grocery_rules3)
inspect(grocery_rules3[400:410])
inspect(grocery_rules3[400:410])
inspect(grocery_rules3[1:10])
inspect(grocery_rules3[201:210])
inspect(grocery_rules3[201:210])
inspect(sort(grocery_rules3))
inspect(sort(grocery_rules3, by = "lift")[1:10])
inspect(sort(grocery_rules3, by = "lift")[1:10])
rm(list = ls())
teens <- read.csv(file = "mlwr/snsdata.csv")
# 데이터 확인
str(teens)
head(teens)
# 몇가지 변수(특징)에서 결측치(NA)가 보임
summary(teens)
# gender 변수의 NA 갯수
table(is.na(teens$gender))
table(teens$gender)
table(teens$gender, useNA = "ifany")
# female 변수를 데이터 프레임에 추가
# 성별이 "F"이고 NA아니면 1, 그렇지 않으면 0을 입력
teens$female <- ifelse(teens$gender == "F" & !is.na(teens$gender),
1, 0)
table(teens$female)
table(teens$female, useNA = "ifany")
# nogender 변수를 데이터 프레임에 추가
# gender 변수가 NA이면 1, 그렇지 않으면 0을 입력
teens$nogender <- ifelse(is.na(teens$geder), 1, 0)
# nogender 변수를 데이터 프레임에 추가
# gender 변수가 NA이면 1, 그렇지 않으면 0을 입력
teens$nogender <- ifelse(is.na(teens$gender), 1, 0)
table(teens$nogender, useNA = "ifany")
# age 변수 확인
summary(teens$age)
summary(teens$age)
# age의 정상 범위는 13 ~ 19라고 가정 -> 이외의 값들은 NA
teens$age <- ifelse(teens$age >= 13 & teens$age <= 19,
teens$age, NA)
summary(teens$age)
# age의 NA들을 gradyear별 age의 평균값으로 대체
# dplyr 패키지 이용
library(dplyr)
teens %>%
group_by(gradyear) %>%
filter(!is.na(age)) %>%
summarise(mean(age))
teens %>%
group_by(gradyear) %>%
summarise(mean(age, na.rm = T))
teens %>%
group_by(gradyear) %>%
summarise(mean(age, na.rm = T))
# 그룹별 평균(또는 임의 함수)를 적용해서 벡터를 리턴하는 함수
# stats::ave(평균을 계산할 벡터, 그룹핑 변수, FUN = mean)
df <- data.frame(class = c(1, 1, 1, 2, 2),
score = c(10, 9, NA, 9, 8))
# 그룹별 평균(또는 임의 함수)를 적용해서 벡터를 리턴하는 함수
# stats::ave(평균을 계산할 벡터, 그룹핑 변수, FUN = mean)
df <- data.frame(class = c(1, 1, 1, 2, 2),
score = c(10, 9, 8, 9, 8))
df
ave(df$score, df$class, FUN = mean)
# 그룹별 평균(또는 임의 함수)를 적용해서 벡터를 리턴하는 함수
# stats::ave(평균을 계산할 벡터, 그룹핑 변수, FUN = mean)
df <- data.frame(class = c(1, 1, 1, 2, 2),
score = c(10, 9, NA, 9, 8))
df
ave(df$score, df$class, FUN = mean)
my_mean <- function(x) {
mean(x, na.rm = T)
}
ave(df$score, df$class, FUN = my_mean)
ave_age <- ave(teens$age, teens$gradyear, FUN = my_mean)
head(ave_age)
tail(ave_age)
teens$age <- ifelse(is.na(teens$age), ave_age, teens$age)
summary(teens$age)
# k-평균 군집화 알고리즘의 모델을 생성
str(teens)
# 개인 식별 정보(gradyear, gender, age, friends)를 제외하고,
# 오로지 관심사들로만 clustering을 시도
interests <- teens[5:40]
stR(interests)
str(interests)
set.seed(2345)
teen_clusters <- kmeans(interests, 5)
str(teen_clusters)
str(teen_clusters$cluster)
table(teen_clusters$cluster)
teens[c("cluster", "gender", "age", "friends")]
teens[1:10, c("cluster", "gender", "age", "friends")]
teens[1:10, c("cluster", "gender", "age", "friends")]
# 데이터 확인
str(teens)
teens[1:10, c("cluster", "gender", "age", "friends")]
# 모델이 분류한 클러스트가 어떤 특징들을 갖고 있을까?
teens$cluster <- teen_clusters$cluster
teens[1:10, c("cluster", "gender", "age", "friends")]
teens[1:10, c("cluster", "gender", "age", "friends")]
teen_clusters$centers
rm(list = ls())
# 데이터 준비
teens <- read.csv(file = "mlwr/snsdata.csv")
# 데이터 확인
str(teens)
table(teens$gender)
table(teens$gender, useNA = "ifany")
summary(teens)
# 결측치 처리 - 새로운 변수 생성
# 성별
teens$female <- ifelse(teens$gender == "F"$ !is.na(teens$gender),
1, 0)
# 결측치 처리 - 새로운 변수 생성
# 성별
teens$female <- ifelse(teens$gender == "F" $ !is.na(teens$gender),
1, 0)
# 결측치 처리 - 새로운 변수 생성
# 성별
teens$female <- ifelse(teens$gender == "F" &!is.na(teens$gender),
1, 0)
table(teens$gender, useNA = "ifany")
table(teens$female, useNA = "ifany")
# NA값 변수 생성
teens$nogender <- iselse(is.na(teens$gender),
1, 0)
# NA값 변수 생성
teens$nogender <- ifelse(is.na(teens$gender),
1, 0)
table(teens$nogender)
sumary(teens)
summary(teens)
# age 변수 확인
summary(teens$age)
# age 정상 범위: 13 ~ 19, 이외 값은 모두 NA 처리
teens$age <- ifelse(teens$age >= 13 & teens$age <=19,
teens$age, NA)
summary(teens$age)
# age의 NA들을 gradyear별 age의 평균값으로 대체
teens %>%
group_by(gradyear) %>%
filter(is.na(age)) %>%
summarise(mean(age))
# age의 NA들을 gradyear별 age의 평균값으로 대체
teens %>%
group_by(gradyear) %>%
filter(!is.na(age)) %>%
summarise(mean(age))
table(teens$female, useNA = "ifany")
table(teens$nogender, useNA = "ifany")
teens %>%
group_by(gradyear) %>%
summarise(mean(age), na.rm = T)
teens %>%
group_by(gradyear) %>%
summarise(mean(age, na.rm = T))
mean(x, na.rm = T)
# 그룹별 평균을 적용해서 벡터를 리턴하는 함수: ave
my_mean <- function(x) {
mean(x, na.rm = T)
}
ave_age <- ave(teens$age, teens$gradyear, FUN = my_mean)
table(ave_age)
summary(ave_age)
str(teens)
# 클러스터링 모델 생성(개인 식별 정보 제외)
interests <- teens[5:40]
set.seed(123)
teen_clusters <- kmeans(interests, 5)
str(teen_clusters)
normalize <- function(x) {
ruturn((x - min(x)) / (max(x) - min(x)))
}
interests_norm <- as.data.frame(lapply(interests, normalize))
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
interests_norm <- as.data.frame(lapply(interests, normalize))
summary(interests_norm)
teen_clusters <- kmeans(interests_norm, 5)
str(teen_clusters)
# 클러스트 특징
teens$cluster <- teen_clusters$cluster
teens[1:10, c("cluster", "gender", "age", "friends")]
teen_clusters$centers
teen_clusters <- kmeans(interests_norm, 5)
# 클러스트 특징
teens$cluster <- teen_clusters$cluster
teens[1:10, c("cluster", "gender", "age", "friends")]
teen_clusters$centers
teen_clusters$size
summary(teen_clusters)
str(teen_clusters)
str(teen_clusters)
summary(interests_norm)
teen_clusters$centers
